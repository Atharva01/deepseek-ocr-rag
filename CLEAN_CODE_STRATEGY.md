# WSRPN-VL: Clean Code Implementation Strategy

**Document Date**: December 24, 2025  
**Status**: Strategy Document (Before Implementation)  
**Objective**: Define clean architecture, code organization, and implementation phases

---

## 1. Current State Analysis

### 1.1 Existing WSRPN Codebase Structure

```
src/
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ backbone/              # Feature extractors (DenseNet, ViT, DINO)
â”‚   â”œâ”€â”€ object_detectors/
â”‚   â”‚   â””â”€â”€ wsrpn.py          # Main WSRPN model
â”‚   â”œâ”€â”€ losses.py             # Existing loss functions (BCE, SupConPerClass)
â”‚   â”œâ”€â”€ model_components.py   # Utility components (attention, pooling)
â”‚   â”œâ”€â”€ model_interface.py    # Abstract base classes
â”‚   â”œâ”€â”€ model_loader.py       # Model instantiation
â”‚   â”œâ”€â”€ soft_roi_pool.py      # Gaussian soft ROI pooling
â”‚   â””â”€â”€ positional_embedding.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ cxr8.py              # CXR8 dataset
â”‚   â”œâ”€â”€ vindr.py             # VinDR dataset
â”‚   â””â”€â”€ datasets.py          # Generic dataset loaders
â”‚
â”œâ”€â”€ conf/                     # Hydra configuration
â”‚   â””â”€â”€ (config yamls)
â”‚
â”œâ”€â”€ train.py                 # Training entry point
â”œâ”€â”€ evaluate.py              # Evaluation entry point
â””â”€â”€ utils/                   # Utility functions
```

### 1.2 Current Dependencies
- PyTorch (model, training)
- Transformers (backbone loaders)
- Hydra (configuration management)
- OpenCV/PIL (image processing)
- NumPy/SciPy (numerical operations)

### 1.3 Key Patterns Observed
```
âœ“ Model registry pattern (decorator-based instantiation)
âœ“ Configuration via Hydra dataclasses
âœ“ Modular loss functions (easy to add new ones)
âœ“ Abstract interfaces (ObjectDetectorModelInterface)
âœ— Limited vision-language integration
âœ— No explicit phase-based training scheduler
âœ— BERT/text encoder not yet integrated
```

---

## 2. Clean Architecture Design

### 2.1 Design Principles

```
PRINCIPLE 1: Single Responsibility
â”œâ”€ Each module has one clear purpose
â”œâ”€ Vision model â‰  Text model â‰  Training loop
â””â”€ Easy to test and maintain

PRINCIPLE 2: Dependency Injection
â”œâ”€ Components receive dependencies as arguments
â”œâ”€ No global state or hard-coded imports
â””â”€ Testable without side effects

PRINCIPLE 3: Separation of Concerns
â”œâ”€ Model architecture â‰  Loss computation â‰  Data loading
â”œâ”€ Phase scheduling â‰  Training mechanics
â””â”€ Inference â‰  Training

PRINCIPLE 4: Interface Contracts
â”œâ”€ Define clear input/output specifications
â”œâ”€ Type hints everywhere
â”œâ”€ Exceptions for contract violations

PRINCIPLE 5: Gradual Adoption
â”œâ”€ VL components orthogonal to existing WSRPN
â”œâ”€ Minimal changes to core model
â”œâ”€ Feature flags for optional components
```

### 2.2 Proposed New Component Structure

**Legend**: 
- ðŸŸ¢ GREEN = Existing (unchanged or minimal changes)
- ðŸ”´ RED = New (to be created)
- ðŸŸ¡ YELLOW = Modified (enhanced with feature flags)

```
src/
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ backbone/                    ðŸŸ¢ EXISTING
â”‚   â”‚   â””â”€â”€ (DenseNet, ViT, DINO loaders - unchanged)
â”‚   â”‚
â”‚   â”œâ”€â”€ vision_language/             ðŸ”´ NEW DIRECTORY â† VL INTEGRATION
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ text_encoder.py          # BERT wrapper (frozen)
â”‚   â”‚   â”œâ”€â”€ vision_projector.py      # Vision â†’ shared space (global, patch, ROI)
â”‚   â”‚   â”œâ”€â”€ text_projector.py        # Text â†’ shared space
â”‚   â”‚   â”œâ”€â”€ vl_branch.py             # Orchestrator (combines text + vision)
â”‚   â”‚   â””â”€â”€ vl_utils.py              # Helper functions (normalization, etc)
â”‚   â”‚
â”‚   â”œâ”€â”€ training/                    ðŸ”´ NEW DIRECTORY â† PHASE MANAGEMENT
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ loss_scheduler.py        # Phase-based loss weight scheduling
â”‚   â”‚   â”œâ”€â”€ metrics_tracker.py       # Multi-phase metrics aggregation
â”‚   â”‚   â””â”€â”€ training_utils.py        # Helper functions (logging, etc)
â”‚   â”‚
â”‚   â”œâ”€â”€ gaussian_losses/             ðŸ”´ NEW DIRECTORY â† GAUSSIAN OPTIMIZATION
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ concentration.py         # Gaussian concentration loss
â”‚   â”‚   â”œâ”€â”€ sparsity.py              # Gaussian sparsity loss
â”‚   â”‚   â”œâ”€â”€ alignment.py             # Box-Gaussian alignment loss
â”‚   â”‚   â””â”€â”€ suppression.py           # Negative region suppression loss
â”‚   â”‚
â”‚   â”œâ”€â”€ object_detectors/            ðŸŸ¡ EXISTING (minor changes)
â”‚   â”‚   â”œâ”€â”€ wsrpn.py                 # EXISTING: minimal modifications
â”‚   â”‚   â”‚                             # - Add use_vl_branch flag
â”‚   â”‚   â”‚                             # - Add use_gaussian_losses flag
â”‚   â”‚   â”‚                             # - Optional VL initialization
â”‚   â”‚   â”‚                             # - Backward compatible (no breaking changes)
â”‚   â”‚   â””â”€â”€ wsrpn_vl.py              # ðŸ”´ NEW: subclass WSRPN with full VL support
â”‚   â”‚
â”‚   â”œâ”€â”€ losses.py                    ðŸŸ¡ MODIFIED
â”‚   â”‚   # - Add ContrastiveVLLoss
â”‚   â”‚   # - Add other VL-specific losses
â”‚   â”‚   # - Existing BCE, SupConPerClass unchanged
â”‚   â”‚
â”‚   â”œâ”€â”€ model_components.py          ðŸŸ¢ EXISTING (unchanged)
â”‚   â”œâ”€â”€ model_interface.py           ðŸŸ¢ EXISTING (unchanged)
â”‚   â”œâ”€â”€ model_loader.py              ðŸŸ¢ EXISTING (unchanged)
â”‚   â”œâ”€â”€ soft_roi_pool.py             ðŸŸ¢ EXISTING (unchanged)
â”‚   â”œâ”€â”€ positional_embedding.py      ðŸŸ¢ EXISTING (unchanged)
â”‚   â””â”€â”€ backbone/                    ðŸŸ¢ EXISTING (unchanged)
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ __init__.py                  ðŸŸ¢ EXISTING (unchanged)
â”‚   â”œâ”€â”€ cxr8.py                      ðŸŸ¢ EXISTING (unchanged)
â”‚   â”œâ”€â”€ vindr.py                     ðŸŸ¢ EXISTING (unchanged)
â”‚   â””â”€â”€ datasets.py                  ðŸŸ¡ MODIFIED (optional)
â”‚                                    # - Add text caption support (optional)
â”‚                                    # - Handle tokenization in collate_fn
â”‚
â”œâ”€â”€ training/                        ðŸ”´ NEW DIRECTORY â† TRAINING ORCHESTRATION
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ train_pipeline.py            # Main training loop (enhanced)
â”‚   â”œâ”€â”€ validators.py                # Input/output validation
â”‚   â””â”€â”€ phase_manager.py             # Phase lifecycle management
â”‚
â”œâ”€â”€ conf/                            ðŸŸ¢ EXISTING (add new yaml files)
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”œâ”€â”€ wsrpn.yaml               ðŸŸ¢ EXISTING
â”‚   â”‚   â”œâ”€â”€ wsrpn_vl.yaml            ðŸ”´ NEW: VL-specific config
â”‚   â”‚   â””â”€â”€ (other configs - unchanged)
â”‚   â”‚
â”‚   â”œâ”€â”€ training/                    ðŸ”´ NEW DIRECTORY
â”‚   â”‚   â”œâ”€â”€ phases.yaml              # Phase definitions (warmup, gaussian, vl)
â”‚   â”‚   â”œâ”€â”€ loss_scheduler.yaml      # Loss weight schedules
â”‚   â”‚   â””â”€â”€ gaussian.yaml            # Gaussian loss config
â”‚   â”‚
â”‚   â””â”€â”€ (other configs - unchanged)
â”‚
â”œâ”€â”€ train.py                         ðŸŸ¢ EXISTING (unchanged for now)
â”‚                                    # Can add train_wsrpn_vl.py later
â”‚
â”œâ”€â”€ evaluate.py                      ðŸŸ¢ EXISTING (unchanged)
â”‚
â”œâ”€â”€ utils/                           ðŸŸ¢ EXISTING
â”‚   â””â”€â”€ (unchanged)
â”‚
â”œâ”€â”€ metrics/                         ðŸŸ¢ EXISTING (unchanged)
â”‚
â”œâ”€â”€ unittests/                       ðŸŸ¢ EXISTING (unchanged)
â”‚
â””â”€â”€ plot/                            ðŸŸ¢ EXISTING (unchanged)
```

### 2.2.1 Structure Comparison: Old vs New

```
BEFORE (Current WSRPN):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
src/
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ object_detectors/wsrpn.py   â† Single detection model
â”‚   â”œâ”€â”€ losses.py                   â† BCE, SupConPerClass only
â”‚   â””â”€â”€ (supporting modules)
â”‚
â”œâ”€â”€ train.py                        â† Single training script
â”‚
â””â”€â”€ data/
    â””â”€â”€ datasets.py                 â† Image + Labels only

Issues:
  âœ— No VL integration
  âœ— No phase-based scheduling
  âœ— No Gaussian optimization losses
  âœ— No structured phase management
  âœ— Hard to add multi-modal learning


AFTER (WSRPN-VL with Clean Architecture):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
src/
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ vision_language/            â† NEW: VL branch (orthogonal to WSRPN)
â”‚   â”‚   â”œâ”€â”€ text_encoder.py        # BERT integration
â”‚   â”‚   â”œâ”€â”€ vision_projector.py    # Shared embedding space
â”‚   â”‚   â”œâ”€â”€ text_projector.py      # Shared embedding space
â”‚   â”‚   â””â”€â”€ vl_branch.py           # Unified VL module
â”‚   â”‚
â”‚   â”œâ”€â”€ training/                   â† NEW: Training infrastructure
â”‚   â”‚   â”œâ”€â”€ loss_scheduler.py      # Phase-based scheduling
â”‚   â”‚   â””â”€â”€ metrics_tracker.py     # Multi-phase metrics
â”‚   â”‚
â”‚   â”œâ”€â”€ gaussian_losses/            â† NEW: Gaussian optimization
â”‚   â”‚   â”œâ”€â”€ concentration.py       # Sharp Gaussians
â”‚   â”‚   â”œâ”€â”€ sparsity.py           # Sparse attention
â”‚   â”‚   â”œâ”€â”€ alignment.py          # Box consistency
â”‚   â”‚   â””â”€â”€ suppression.py        # False positive reduction
â”‚   â”‚
â”‚   â”œâ”€â”€ object_detectors/
â”‚   â”‚   â”œâ”€â”€ wsrpn.py              â† MINIMAL CHANGES (backward compatible)
â”‚   â”‚   â””â”€â”€ wsrpn_vl.py           â† NEW: Full VL support
â”‚   â”‚
â”‚   â”œâ”€â”€ losses.py                 â† ADD: ContrastiveVLLoss
â”‚   â””â”€â”€ (existing modules)
â”‚
â”œâ”€â”€ training/                        â† NEW: Training orchestration
â”‚   â”œâ”€â”€ train_pipeline.py          # Enhanced training loop
â”‚   â””â”€â”€ validators.py              # Input validation
â”‚
â”œâ”€â”€ conf/
â”‚   â”œâ”€â”€ model/wsrpn_vl.yaml       â† NEW: VL configuration
â”‚   â””â”€â”€ training/                  â† NEW: Phase config
â”‚
â””â”€â”€ data/
    â””â”€â”€ datasets.py               â† OPTIONAL: text support


Benefits:
  âœ“ Clean separation: VL orthogonal to detection
  âœ“ Modular: Each loss independently testable
  âœ“ Backward compatible: Original WSRPN still works
  âœ“ Extensible: Easy to add new phases/losses
  âœ“ Type-safe: Full type hints everywhere
  âœ“ Reproducible: Centralized scheduling


KEY CHANGES SUMMARY:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. NEW directories: vision_language/, training/, gaussian_losses/
2. NEW classes: ~12 classes (text encoder, projectors, losses, scheduler)
3. NEW files: ~12 new files (no deletions)
4. MODIFIED files: 3 files (wsrpn.py, losses.py, conf yamls)
5. UNCHANGED files: All existing model/data/utils files

Impact Analysis:
  â”œâ”€ Lines of code added: ~2500 new lines
  â”œâ”€ Lines of code modified: ~100 lines in existing files
  â”œâ”€ Lines of code deleted: 0 (backward compatible)
  â”œâ”€ Breaking changes: 0 (feature flags used)
  â””â”€ Existing functionality: 100% preserved
```

### 2.3 Module Dependency Graph

```
         [Training Pipeline]
              â†“ uses
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â†“                    â†“
[Phase Manager]    [Loss Scheduler]
    â†“                    â†“
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“ coordinates
         [WSRPN-VL Model]
              â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â†“         â†“                  â†“
 [Backbone][VL Branch]    [Gaussian Losses]
   (Frozen)   â”œâ”€ Vision Encoder     â”œâ”€ Concentration
              â”œâ”€ Text Encoder      â”œâ”€ Sparsity
              â”œâ”€ Projectors        â””â”€ Alignment
              â””â”€ Contrastive Loss

Data flows: Image + Text â†’ Model â†’ Losses â†’ Gradients â†’ Optimizer
```

---

## 3. Clean Code Implementation Strategy

### 3.1 File Creation Plan (Minimal Changes)

**PHASE A: Core VL Components (No WSRPN modification)**

```
FILE 1: src/model/vision_language/text_encoder.py
Purpose: BERT wrapper with frozen parameters
Content:
  - class TextEncoderBERT
  - Input: tokenized text (input_ids, attention_mask)
  - Output: embeddings (B, d_hidden)
  - Signature: forward(input_ids, attention_mask) â†’ Tensor
  - Type hints: All inputs/outputs typed
  - Config: Model name, hidden_size, freeze flag

FILE 2: src/model/vision_language/vision_projector.py
Purpose: Project vision features to shared space
Content:
  - class VisionProjector (for whole image)
  - class PatchVisionProjector (for patch features)
  - class ROIVisionProjector (for ROI features)
  - Input: feature tensor (B, d) or (B, K, d)
  - Output: normalized embeddings (B, d_shared) or (B, K, d_shared)
  - Configuration: input_dim, output_dim, hidden_dims

FILE 3: src/model/vision_language/text_projector.py
Purpose: Project text embeddings to shared space
Content:
  - class TextProjector
  - Input: text embeddings (B, 768)
  - Output: normalized embeddings (B, d_shared)
  - Configuration: input_dim, output_dim, hidden_dims

FILE 4: src/model/vision_language/vl_branch.py
Purpose: Unified VL branch combining text + vision
Content:
  - class VisionLanguageBranch
  - Input: image features, image labels, text + tokenizer
  - Forward: Orchestrate encoders + projectors
  - Output: vision_embeddings, text_embeddings (normalized, on shared space)
  - Initialization: Load pre-trained BERT, initialize projectors

FILE 5: src/model/training/loss_scheduler.py
Purpose: Phase-based loss weight scheduling
Content:
  - class LossWeightScheduler
  - Methods:
    * get_phase(step: int) â†’ (phase_name: str, step_in_phase: int)
    * get_weights(step: int) â†’ Dict[loss_name, weight]
    * get_phase_info(step: int) â†’ DetailedPhaseInfo
  - Configuration: Phase definitions (name, step_range, weight_profiles)
  - Phases: Warmup (0-N), Gaussian (N-M), VL (M-T)
  - State tracking: Current phase, transitions, upcoming changes

FILE 6: src/model/gaussian_losses/concentration.py
Purpose: Gaussian concentration loss (peak sharpness)
Content:
  - class GaussianConcentrationLoss
  - Input: Gaussian parameters (center, scale), target locations
  - Output: scalar loss value
  - Metric: Entropy of Gaussian map
  - Interpretability: Lower â†’ sharper attention

FILE 7: src/model/gaussian_losses/sparsity.py
Purpose: Gaussian sparsity loss (region-focused)
Content:
  - class GaussianSparsityLoss
  - Input: Gaussian parameters, image features
  - Output: scalar loss value
  - Metric: Attention mass outside 3-sigma region
  - Interpretability: Lower â†’ more sparse

FILE 8: src/model/gaussian_losses/alignment.py
Purpose: Box-Gaussian parameter alignment
Content:
  - class BoxGaussianAlignmentLoss
  - Input: Predicted boxes, Gaussian parameters
  - Output: scalar loss value
  - Ensures: Consistency between representations

FILE 9: src/model/gaussian_losses/suppression.py
Purpose: Suppress false positives in normal regions
Content:
  - class NegativeRegionSuppressionLoss
  - Input: Predictions in known normal regions, soft labels
  - Output: scalar loss value
  - Use case: Reduce activations in "definitely normal" areas
```

**PHASE B: Training & Scheduling (Minimal WSRPN changes)**

```
FILE 10: src/training/phase_manager.py
Purpose: Lifecycle management for multi-phase training
Content:
  - class PhaseManager
  - State machine: Warmup â†’ Gaussian â†’ VL
  - Methods:
    * start_phase(phase_name)
    * is_phase_transition(step)
    * get_current_phase()
  - Logging: Print phase transitions to console + log file

FILE 11: src/training/train_pipeline.py
Purpose: Enhanced training loop with phase support
Content:
  - function: train_wsrpn_vl(config, model, dataloaders, device)
  - Loop structure:
    * FOR each step:
      - Get loss weights from scheduler
      - Forward pass (image + text)
      - Compute all losses
      - Weighted sum
      - Backward pass
      - Optimizer step
  - Logging: Separate tracking per loss component
  - Checkpointing: Save best + latest

FILE 12: src/training/validators.py
Purpose: Input validation and contract enforcement
Content:
  - def validate_batch(batch) â†’ bool
  - def validate_config(config) â†’ bool
  - def validate_model_output(output) â†’ bool
  - Error messages: Clear, actionable feedback
```

**PHASE C: WSRPN Augmentation (Minimal modifications)**

```
MODIFIED FILE: src/model/object_detectors/wsrpn.py
Changes: Backward compatible, feature-flag based
  â”œâ”€ Add use_vl_branch: bool parameter
  â”œâ”€ Add use_gaussian_losses: bool parameter
  â”œâ”€ If use_vl_branch=true:
  â”‚  â”œâ”€ Initialize VisionLanguageBranch
  â”‚  â”œâ”€ Extend forward() to handle text input
  â”‚  â””â”€ Return VL embeddings alongside predictions
  â”œâ”€ If use_gaussian_losses=true:
  â”‚  â”œâ”€ Initialize Gaussian loss functions
  â”‚  â””â”€ Include in loss computation
  â””â”€ Else: Behave like original WSRPN (no breaking changes)

STRATEGY: Inheritance with optional mixins
  class WSRPNBase (original)
  class WSRPN_VL(WSRPNBase):  â† VL components added here
```

### 3.2 Clean Code Principles Applied

**PRINCIPLE: Type Hints Everywhere**

```python
# âœ“ GOOD: Clear contracts
def forward(
    self,
    images: Tensor,                      # (B, 1, 224, 224)
    labels: Tensor,                      # (B, 14)
    text_tokens: Optional[Dict] = None,  # Optional for backward compat
    step: int = 0
) -> Dict[str, Any]:
    """
    Forward pass with optional VL.
    
    Args:
        images: Batch of X-ray images
        labels: Ground truth binary labels
        text_tokens: Tokenized text descriptions (optional)
        step: Current training step (for phase scheduling)
    
    Returns:
        {
            'predictions': Tensor,        # Predictions
            'losses': Dict[str, Tensor],  # Loss components
            'vl_embeddings': Optional[Dict],  # If VL enabled
            'metrics': Dict[str, float]   # Evaluation metrics
        }
    """
    ...

# âœ— BAD: Ambiguous
def forward(self, x, y, z=None, s=0):
    ...
```

**PRINCIPLE: Single Responsibility**

```python
# âœ“ GOOD: Each class does one thing
class VisionProjector(nn.Module):
    """Project vision features to shared embedding space."""
    def forward(self, features: Tensor) -> Tensor:
        return normalized_embeddings

class TextProjector(nn.Module):
    """Project text embeddings to shared embedding space."""
    def forward(self, embeddings: Tensor) -> Tensor:
        return normalized_embeddings

# âœ— BAD: Mixed responsibilities
class VLModule(nn.Module):
    def forward(self, images, text):
        # Projects vision, text, computes loss, updates metrics, logs...
        # Too much!
```

**PRINCIPLE: Dependency Injection**

```python
# âœ“ GOOD: Dependencies passed in
class LossScheduler:
    def __init__(self, phase_config: PhaseConfig):
        self.phases = phase_config.phases
    
    def get_weights(self, step: int) -> Dict[str, float]:
        ...

scheduler = LossScheduler(config.training.phases)
weights = scheduler.get_weights(current_step)

# âœ— BAD: Hard-coded dependencies
class LossScheduler:
    def get_weights(self, step: int) -> Dict[str, float]:
        # What phases? Hard-coded to specific training strategy
        if step < 1000:
            return {'detection': 1.0, 'vl': 0.0}
        ...
```

**PRINCIPLE: Clear Contracts (Interfaces)**

```python
# âœ“ GOOD: Protocol-based (Python 3.8+)
from typing import Protocol

class TextEncoder(Protocol):
    def forward(
        self,
        input_ids: Tensor,
        attention_mask: Tensor
    ) -> Tensor:
        """Returns (B, hidden_size) embeddings."""
        ...

# Any class implementing this protocol works
class BERTEncoder:
    def forward(self, input_ids: Tensor, attention_mask: Tensor) -> Tensor:
        return self.bert(input_ids, attention_mask)[1]  # [CLS] pooling

# âœ— BAD: Loose coupling
def compute_vl_loss(text_model):
    # What does text_model have? No idea!
    result = text_model.something()
```

**PRINCIPLE: Configuration Objects**

```python
# âœ“ GOOD: Dataclass-based configuration
from dataclasses import dataclass

@dataclass
class VLConfig:
    use_vl_branch: bool = True
    text_model: str = "bert-base-uncased"
    shared_dim: int = 128
    freeze_text_encoder: bool = True
    temperature: float = 0.15

@dataclass
class GaussianConfig:
    use_concentration_loss: bool = True
    use_sparsity_loss: bool = True
    concentration_weight: float = 0.3
    sparsity_weight: float = 0.3

# Usage: config.vl.use_vl_branch, config.gaussian.concentration_weight
# Type-safe, IDE-friendly, validation-friendly

# âœ— BAD: String-based configuration
config = {
    'vl_branch': True,
    'text_model': 'bert',
    'shared_dim': '128',  # Oops, string instead of int
    'freeze_encoder': 'true'  # Hard to parse
}
```

---

## 4. Implementation Phases

### 4.1 Phase ZERO: Setup & Preparation (1-2 days)

**Objective**: Foundation for clean implementation

Tasks:
- [ ] Create VL-specific directories (vision_language/, training/, gaussian_losses/)
- [ ] Setup type checking (mypy configuration)
- [ ] Create test infrastructure
- [ ] Document module interfaces
- [ ] Setup logging (structured logging)

Success Criteria:
- âœ“ Directory structure matches design
- âœ“ Type checking configured
- âœ“ Test runners operational
- âœ“ Logging structured

### 4.2 Phase ONE: Core VL Components (2-3 days)

**Objective**: Implement VL branch independently

Tasks:
- [ ] Implement TextEncoderBERT (src/model/vision_language/text_encoder.py)
- [ ] Implement VisionProjector variants (src/model/vision_language/vision_projector.py)
- [ ] Implement TextProjector (src/model/vision_language/text_projector.py)
- [ ] Create VisionLanguageBranch orchestrator
- [ ] Write unit tests for each component
- [ ] Integration test: VL branch standalone

Success Criteria:
- âœ“ All unit tests pass
- âœ“ Forward pass works with sample inputs
- âœ“ Output dimensions correct
- âœ“ Type hints complete
- âœ“ Docstrings comprehensive

### 4.3 Phase TWO: Loss Scheduling & Gaussian Losses (2-3 days)

**Objective**: Implement training infrastructure

Tasks:
- [ ] Implement LossWeightScheduler
- [ ] Implement PhaseManager
- [ ] Implement GaussianConcentrationLoss
- [ ] Implement GaussianSparsityLoss
- [ ] Implement BoxGaussianAlignmentLoss
- [ ] Write tests for scheduler (phase transitions)
- [ ] Write tests for loss functions

Success Criteria:
- âœ“ Phase transitions occur at correct steps
- âœ“ Loss weights match specifications
- âœ“ All Gaussian losses return scalar tensors
- âœ“ Gradient flow verified

### 4.4 Phase THREE: WSRPN Integration (2-3 days)

**Objective**: Connect VL to existing WSRPN

Tasks:
- [ ] Create WSRPN_VL variant (inheritance-based)
- [ ] Add feature flags (use_vl_branch, use_gaussian_losses)
- [ ] Modify forward() for optional text input
- [ ] Extend loss computation
- [ ] Backward compatibility tests (original WSRPN still works)
- [ ] Integration tests (full model with VL)

Success Criteria:
- âœ“ Original WSRPN unchanged (backward compatible)
- âœ“ WSRPN_VL adds VL without side effects
- âœ“ Feature flags work correctly
- âœ“ All tests pass

### 4.5 Phase FOUR: Training Pipeline (2-3 days)

**Objective**: Implement train_wsrpn_vl() training loop

Tasks:
- [ ] Implement train_pipeline.py
- [ ] Add phase detection + logging
- [ ] Add metrics tracking (per-phase)
- [ ] Add checkpointing (best + latest)
- [ ] Add validation loop
- [ ] Implement validators.py
- [ ] Write integration tests

Success Criteria:
- âœ“ Training loop runs for 10 steps without errors
- âœ“ Metrics tracked correctly
- âœ“ Checkpoints saved with correct format
- âœ“ Phase transitions logged clearly

---

## 5. Code Quality Standards

### 5.1 Type Hints

**MANDATORY**: All functions, methods, class attributes

```python
# âœ“ Required style
def compute_loss(
    predictions: Tensor,           # (B, C)
    targets: Tensor,               # (B, C)
    weights: Optional[Tensor] = None  # (B,)
) -> Tensor:  # scalar
    ...

class VLBranch(nn.Module):
    vision_projector: nn.Module
    text_encoder: TextEncoder
    
    def forward(
        self,
        images: Tensor,
        text_tokens: Dict[str, Tensor]
    ) -> Tuple[Tensor, Tensor]:  # (vision_emb, text_emb)
        ...
```

### 5.2 Docstrings

**MANDATORY**: Google-style for all public methods

```python
def compute_contrastive_loss(
    vision_embeddings: Tensor,
    text_embeddings: Tensor,
    temperature: float = 0.15
) -> Tensor:
    """Compute NT-Xent contrastive loss between vision and text.
    
    Aligns vision features with text embeddings using normalized
    temperature-scaled cross-entropy loss (SimCLR style).
    
    Args:
        vision_embeddings: Vision feature embeddings (B, D). Should be
            L2-normalized.
        text_embeddings: Text feature embeddings (B, D). Should be
            L2-normalized.
        temperature: Scaling temperature for softmax. Default 0.15.
            Lower â†’ sharper contrast.
    
    Returns:
        Scalar loss value (averaged over batch).
    
    Raises:
        ValueError: If embeddings not normalized or shapes don't match.
    
    Example:
        >>> v_emb = torch.randn(32, 128)
        >>> t_emb = torch.randn(32, 128)
        >>> v_emb = F.normalize(v_emb, p=2, dim=1)
        >>> t_emb = F.normalize(t_emb, p=2, dim=1)
        >>> loss = compute_contrastive_loss(v_emb, t_emb)
    """
    ...
```

### 5.3 Testing

**MANDATORY**: Unit test for every class/function

```
tests/
â”œâ”€â”€ test_text_encoder.py
â”œâ”€â”€ test_vision_projector.py
â”œâ”€â”€ test_vl_branch.py
â”œâ”€â”€ test_loss_scheduler.py
â”œâ”€â”€ test_gaussian_losses.py
â”œâ”€â”€ test_wsrpn_vl.py
â””â”€â”€ test_train_pipeline.py

Each test file:
  â”œâ”€â”€ Fixture for common test data
  â”œâ”€â”€ Test: initialization with valid config
  â”œâ”€ Test: forward pass with shape validation
  â”œâ”€ Test: backward pass (gradient flow)
  â”œâ”€ Test: edge cases (empty batch, None input)
  â””â”€ Test: error cases (invalid input)
```

### 5.4 Logging

**MANDATORY**: Structured logging for all key operations

```python
import logging

logger = logging.getLogger(__name__)

# âœ“ GOOD: Structured, informative
logger.info(
    "Phase transition",
    extra={
        'from_phase': 'warmup',
        'to_phase': 'gaussian',
        'step': 1000,
        'remaining_steps': 3000
    }
)

logger.warning(
    "Loss diverging",
    extra={
        'loss_value': 2.5,
        'threshold': 1.5,
        'step': 500,
        'recommendation': 'Reduce learning rate'
    }
)

# âœ— BAD: Unstructured, hard to parse
print("Switching to gaussian phase at step 1000")
print("WARNING: loss is high!")
```

### 5.5 Error Handling

**MANDATORY**: Validate inputs, raise clear exceptions

```python
# âœ“ GOOD: Clear validation + actionable errors
def forward(self, images: Tensor, text_tokens: Dict[str, Tensor]):
    if images.shape[1] != 1:
        raise ValueError(
            f"Expected grayscale images (B, 1, H, W), "
            f"got shape {images.shape}. "
            f"Hint: Convert to single channel with torch.mean()"
        )
    
    if images.shape[-1] != 224 or images.shape[-2] != 224:
        raise ValueError(
            f"Expected 224x224 images, got {images.shape[-2:]}. "
            f"Hint: Resize with transforms.Resize((224, 224))"
        )
    
    if 'input_ids' not in text_tokens:
        raise ValueError(
            f"text_tokens missing 'input_ids'. "
            f"Got keys: {list(text_tokens.keys())}. "
            f"Hint: Use tokenizer() to prepare input"
        )
    
    # ... computation ...

# âœ— BAD: Silent failures
def forward(self, images, text_tokens):
    if images.ndim != 4:
        images = images.unsqueeze(0)  # Implicit assumption!
    
    text_emb = text_tokens.get('input_ids', None)
    # What if 'input_ids' is missing? Silent None?
```

---

## 6. Key Design Decisions

### Decision 1: VL Components are Orthogonal

**Choice**: VL branch separate from WSRPN core

**Rationale**:
- âœ“ WSRPN can train without VL (backward compatible)
- âœ“ VL can be tested independently
- âœ“ Easy to toggle on/off with feature flags
- âœ“ Minimal changes to existing code

**Alternative Rejected**: Tightly integrate VL into WSRPN
- âœ— Breaking changes to existing code
- âœ— Harder to debug issues
- âœ— Can't disable VL if needed

### Decision 2: Phase Scheduling is Centralized

**Choice**: LossWeightScheduler controls all phase logic

**Rationale**:
- âœ“ Single source of truth
- âœ“ Easy to experiment with phase boundaries
- âœ“ Reproducible phase transitions
- âœ“ Logging/debugging simpler

**Alternative Rejected**: Phase logic distributed in training loop
- âœ— Hard-coded if/else statements
- âœ— Difficult to modify phase schedule
- âœ— Validation scattered

### Decision 3: Gaussian Losses are Separate Classes

**Choice**: One loss class per Gaussian optimization component

**Rationale**:
- âœ“ Each loss independently testable
- âœ“ Easy to enable/disable specific losses
- âœ“ Clear responsibility
- âœ“ Reusable in other models

**Alternative Rejected**: Monolithic GaussianLoss class
- âœ— Hard to debug individual components
- âœ— Can't enable sparsity without concentration
- âœ— Tightly coupled

### Decision 4: Inheritance for WSRPN_VL

**Choice**: Create WSRPN_VL(WSRPN) subclass

**Rationale**:
- âœ“ Original WSRPN unchanged
- âœ“ Code reuse (inherits all base functionality)
- âœ“ Easy to compare baseline vs VL
- âœ“ Polymorphic (both treated as detectors)

**Alternative Rejected**: Monkeypatch existing WSRPN
- âœ— Pollutes WSRPN namespace
- âœ— Hard to understand what's modified
- âœ— Fragile to base class changes

---

## 7. Validation & Testing Strategy

### 7.1 Unit Testing

```
Each module tested independently:

âœ“ TextEncoderBERT
  â”œâ”€ Load pre-trained BERT
  â”œâ”€ Forward pass shape check
  â”œâ”€ Gradient flow (if unfrozen)
  â”œâ”€ Token sequence handling
  â””â”€ Batch vs single sample

âœ“ VisionProjector
  â”œâ”€ Shape transformation
  â”œâ”€ Normalization (L2)
  â”œâ”€ Gradient flow
  â””â”€ Edge case: batch_size=1

âœ“ LossScheduler
  â”œâ”€ Phase transitions at correct steps
  â”œâ”€ Weight values match spec
  â”œâ”€ Boundary conditions (step=0, step=max)
  â””â”€ Error handling (invalid step)

âœ“ GaussianLosses
  â”œâ”€ Loss returns scalar
  â”œâ”€ Loss â‰¥ 0
  â”œâ”€ Gradient flow
  â”œâ”€ Edge cases (all-zero input, saturated input)
  â””â”€ Dimensionality checks
```

### 7.2 Integration Testing

```
Component combinations:

âœ“ VL Branch End-to-End
  â”œâ”€ Image + text â†’ embeddings
  â”œâ”€ Embedding shapes correct
  â”œâ”€ Embeddings L2-normalized
  â”œâ”€ Batch processing
  â””â”€ Backward pass works

âœ“ WSRPN + VL
  â”œâ”€ Original WSRPN works (use_vl_branch=False)
  â”œâ”€ WSRPN_VL works (use_vl_branch=True)
  â”œâ”€ Predictions unchanged (deterministic)
  â”œâ”€ VL outputs correct format
  â””â”€ Loss computation includes VL

âœ“ Training Loop
  â”œâ”€ 10-step training run without errors
  â”œâ”€ Loss decreasing (or at least not NaN)
  â”œâ”€ Checkpoints saved correctly
  â”œâ”€ Phase transitions logged
  â””â”€ Metrics tracked per phase
```

### 7.3 Smoke Tests (Quick Validation)

```bash
# 1. Import all modules
python -c "from src.model.vision_language import *; print('âœ“')"

# 2. Create model instance
python -c "
from src.model.object_detectors.wsrpn_vl import WSRPN_VL
model = WSRPN_VL(config)
print(f'âœ“ Model created: {model}')"

# 3. Forward pass
python -c "
import torch
from src.model.object_detectors.wsrpn_vl import WSRPN_VL
model = WSRPN_VL(config)
x = torch.randn(2, 1, 224, 224)
out = model(x)
print(f'âœ“ Forward pass output keys: {out.keys()}')"

# 4. Loss computation
python -c "
import torch
from src.model.losses import ContrastiveVLLoss
loss_fn = ContrastiveVLLoss()
v_emb = torch.randn(4, 128)
t_emb = torch.randn(4, 128)
loss = loss_fn(v_emb, t_emb)
print(f'âœ“ Loss: {loss.item():.4f}')"
```

---

## 8. Checklist for Implementation

### Pre-Implementation
- [ ] Review current WSRPN code (wsrpn.py, train.py)
- [ ] Understand model registry pattern
- [ ] Understand Hydra configuration
- [ ] Setup type checking (mypy)
- [ ] Create test infrastructure
- [ ] Document this strategy with team

### Phase ZERO: Setup
- [ ] Create directory structure
- [ ] Create __init__.py files
- [ ] Create test directories
- [ ] Setup logging configuration
- [ ] Document module interfaces

### Phase ONE: VL Components
- [ ] Implement TextEncoderBERT
  - [ ] Load BERT model
  - [ ] Handle tokenization
  - [ ] Freeze parameters
  - [ ] Test with sample inputs
  
- [ ] Implement VisionProjector
  - [ ] Project global features
  - [ ] L2 normalization
  - [ ] Test shapes
  
- [ ] Implement TextProjector
  - [ ] Project text embeddings
  - [ ] L2 normalization
  - [ ] Test shapes
  
- [ ] Create VisionLanguageBranch
  - [ ] Orchestrate encoders
  - [ ] Test end-to-end
  
- [ ] Write unit tests for all components

### Phase TWO: Training Infrastructure
- [ ] Implement LossWeightScheduler
  - [ ] Define phase transitions
  - [ ] Test phase boundaries
  
- [ ] Implement Gaussian losses
  - [ ] Concentration loss
  - [ ] Sparsity loss
  - [ ] Alignment loss
  - [ ] Test each independently
  
- [ ] Implement PhaseManager
  - [ ] State machine logic
  - [ ] Transition logging

### Phase THREE: WSRPN Integration
- [ ] Create WSRPN_VL variant
  - [ ] Inherit from WSRPN
  - [ ] Add VL initialization
  - [ ] Extend forward()
  - [ ] Extend loss computation
  
- [ ] Backward compatibility testing
  - [ ] Original WSRPN unchanged
  - [ ] Feature flags work
  
- [ ] Integration tests

### Phase FOUR: Training Pipeline
- [ ] Implement train_wsrpn_vl()
  - [ ] Phase detection
  - [ ] Loss computation (weighted)
  - [ ] Gradient updates
  - [ ] Metrics tracking
  
- [ ] Add validation loop
  - [ ] Validation metrics
  - [ ] Best checkpoint saving
  
- [ ] Input validators
  - [ ] Batch validation
  - [ ] Config validation
  
- [ ] Full integration test

### Post-Implementation
- [ ] Run full test suite
- [ ] Type checking (mypy)
- [ ] Code review
- [ ] Documentation update
- [ ] Performance profiling
- [ ] Smoke tests on sample data

---

## 9. Summary

### Clean Architecture Benefits

```
âœ“ Maintainability: Clear separation of concerns
âœ“ Testability: Each component independently testable
âœ“ Reusability: Gaussian losses work with other detectors
âœ“ Extensibility: Easy to add new phases/losses
âœ“ Debuggability: Type hints + logging make bugs obvious
âœ“ Reproducibility: Deterministic, well-documented
âœ“ Collaboration: Clear interfaces, minimal merge conflicts
```

### Key Principles

```
1. Orthogonal Components: VL âŠ¥ WSRPN core
2. Centralized Scheduling: One source of truth for phases
3. Dependency Injection: No global state
4. Type Safety: All functions type-hinted
5. Comprehensive Testing: Every class has tests
6. Clear Contracts: Interfaces define expectations
7. Structured Logging: Debug-friendly diagnostics
```

### Timeline Estimate

```
Phase ZERO: Setup                   1-2 days
Phase ONE: VL Components            2-3 days
Phase TWO: Loss & Scheduling        2-3 days
Phase THREE: WSRPN Integration      2-3 days
Phase FOUR: Training Pipeline       2-3 days
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total:                              11-17 days

With expert developer: 9-12 days
With debugging/iteration: 15-20 days
```

---

**Status**: Ready for Implementation  
**Next Step**: Begin Phase ZERO (Setup & Preparation)  
**Validation**: All design decisions documented with rationale
